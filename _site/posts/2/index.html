<!DOCTYPE html>
<html lang="en">
<link rel="icon" type="image/x-icon" href="/favicon.ico?">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Brian Austin</title>
  <meta name="description" content="Long on data.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/posts/2/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Brian Austin" href="http://localhost:4000/feed.xml">

  


  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="bda">
  <meta name="twitter:title" content="Brian Austin">
  <meta name="twitter:description" content="Long on data.">
  
    <meta name="twitter:creator" content="bda">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-NNNNNNNN-N', 'auto');
    ga('send', 'pageview');

  </script>


  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


</head>


  <body>

    <style>
.dropdown {
  position: relative;
  display: inline-block;
}

.dropdown-content {
  display: none;
  position: absolute;
  background-color: #f9f9f9;
  min-width: 160px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
  padding: 12px 16px;
  z-index: 1;
}

.dropdown:hover .dropdown-content {
  display: block;
}
</style>

<header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/about">Brian Austin</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/">Blog</a>
      
        
        <a class="page-link" href="/portfolio/">Portfolio</a>
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Blog Archives</a>
      
        
        <a class="page-link" href="https://www.twitter.com/bda"><h2 class="fa fa-twitter"></h2></a>
      
        
        <a class="page-link" href="https://www.linkedin.com/in/briandaustin/"><h2 class="fa fa-linkedin"></h2></a>
      
        
        <a class="page-link" href="https://www.github.com/austinbrian"><h2 class="fa fa-github"></h2></a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/mood/">#mood</a>
          </h1>

          <p class="post-meta">Apr 4, 2017</p>
        </header>

        <div class="post-content">
          <p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/theo_confetti_angel.jpg" alt="Theo" /></p>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/ncaa_overfitting/">Overfitting the Sweet 16</a>
          </h1>

          <p class="post-meta">Apr 3, 2017</p>
        </header>

        <div class="post-content">
          <p>I have a very simple model for about a quarter of my NCAA bracket predictions: if UNC, move to next round.</p>

<p>This year, that model has worked pretty well.</p>

<p>But unfortunately, it only helps you out in a few circumstances. Every pick is UNC, which is useful when UNC plays Butler, but not so much when Michigan plays Oregon. So since it gives you the same output every time, no matter who is playing, statisticians and data scientists would say this model has <strong>low variance</strong>.</p>

<p>This is isn’t very useful for the broader world of NCAA tournament basketball, so we need a more complex model, one that describes more teams. But here’s the problem a lot of people have in building models: <em>they use everything they know</em>. Let me explain.</p>

<p>Here is the bracket of the Sweet 16 games:</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/ncaa_ss_bracket_8blank.png" alt="where's Dook?" /></p>

<p>Since we currently live in a world where we know who won the Sweet 16 games, we can make a <em>super</em> accurate model to “predict” the winners of those games. It might look like this:</p>
<ol>
  <li>If UNC is playing, pick UNC (this is my model, after all)</li>
  <li>If a school has multiple NCAA championships over the last 20 years, pick it if it has more than the other school</li>
  <li>In a contest between schools in states near large bodies of saltwater and freshwater, pick saltwater</li>
  <li>If teams are both from the Confederacy, pick the eastern-most team</li>
  <li>If a school has an X in its name, pick it (<em>Xs are cool</em>)</li>
  <li>If all else fails, chalk (i.e., pick the team with the lower seed)</li>
</ol>

<p>And we’ll say that these are applied in this order, so that the 1st element of the model trumps the 2nd and so on.</p>

<p>Here is how our picks look for the Sweet 16. It’s a weird model, but I’ve got a good feeling about it.</p>

<table>
  <thead>
    <tr>
      <th>Factor</th>
      <th>Factor Description</th>
      <th>Team Picked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.</td>
      <td>Pick UNC</td>
      <td>UNC</td>
    </tr>
    <tr>
      <td>2.</td>
      <td>NCAA champions</td>
      <td>Florida, Kentucky</td>
    </tr>
    <tr>
      <td>3.</td>
      <td>Saltwater over freshwater</td>
      <td>Florida, Oregon</td>
    </tr>
    <tr>
      <td>4.</td>
      <td>Easternmost Confederate</td>
      <td>S. Carolina</td>
    </tr>
    <tr>
      <td>5.</td>
      <td>Has an X</td>
      <td>Xavier</td>
    </tr>
    <tr>
      <td>6.</td>
      <td>Chalk</td>
      <td>Kansas, Gonzaga</td>
    </tr>
  </tbody>
</table>

<p>How’d we do?</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/NCAA_ss_circles.png" alt="wow such prediction" /></p>

<p>Crushed it.</p>

<p>We got them all right! We are 100% accurate. This is the best model of all time. So we confidently deploy it to predict the winners of the Elite Eight who will move on to the Final Four.</p>

<table>
  <thead>
    <tr>
      <th>Factor</th>
      <th>Factor Description</th>
      <th>Team Picked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.</td>
      <td>Pick UNC</td>
      <td>UNC</td>
    </tr>
    <tr>
      <td>2.</td>
      <td>NCAA champions</td>
      <td>Florida</td>
    </tr>
    <tr>
      <td>3.</td>
      <td>Saltwater over freshwater</td>
      <td><em>N/A, freshwaters already eliminated</em></td>
    </tr>
    <tr>
      <td>4.</td>
      <td>Easternmost Confederate</td>
      <td><em>N/A, Florida already picked</em></td>
    </tr>
    <tr>
      <td>5.</td>
      <td>Has an X</td>
      <td>Xavier</td>
    </tr>
    <tr>
      <td>6.</td>
      <td>Chalk</td>
      <td>Kansas</td>
    </tr>
  </tbody>
</table>

<p>And just as we suspected…</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/NCAA_ss_xxxs.png" alt="not as good" /></p>

<p>Wait what?</p>

<p>The only school we got right here was UNC, giving our model an accuracy rate of 25%. Dang.</p>

<p>What went wrong?</p>

<p>When we went to create a model, we focused on hitting all the points we knew we needed to hit, which statisticians and data scientists refer to as having <strong>high bias</strong>.</p>

<p>This is a big problem in data science, known as “overfitting.” All that means is that predictions overly closely predict the original dataset, and aren’t flexible enough to be applied in the world.</p>

<p>So in summary: predictive accuracy comes at a tradeoff, called <strong>bias</strong>. Simpler models minimize this bias, but run into limitations to accuracy due to low <strong>variance</strong>. Good models are those that minimize both aspects of this.</p>

<p>And Go Heels.</p>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/dc-dogs/">Who&#39;s that Lady?</a>
          </h1>

          <p class="post-meta">Mar 27, 2017</p>
        </header>

        <div class="post-content">
          <p>Let’s say you’re at a local dog park in DC, and you see a dog chasing a ball, doing dog things, and you want to call out for her to come play with you. What should you yell?</p>

<p>Try “Lady” or “Blue.”</p>

<p><img src="https://pbs.twimg.com/media/C77GgWCW0AAAHqi.jpg:large" alt="Peanut! from @darth" /></p>

<p>According to a list of 1,533 dog registrations in the District in 2016, those are most likely names for female and male dogs, respectively. Here are other common pup names.</p>

<table>
  <thead>
    <tr>
      <th>Good Boys</th>
      <th>#</th>
      <th> </th>
      <th>Good Girls</th>
      <th>#</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Blue</td>
      <td>10</td>
      <td> </td>
      <td>Lady</td>
      <td>14</td>
    </tr>
    <tr>
      <td>King</td>
      <td>8</td>
      <td> </td>
      <td>Chloe</td>
      <td>11</td>
    </tr>
    <tr>
      <td>Rocky</td>
      <td>7</td>
      <td> </td>
      <td>Bella</td>
      <td>10</td>
    </tr>
    <tr>
      <td>Charlie</td>
      <td>6</td>
      <td> </td>
      <td>Sheba</td>
      <td>7</td>
    </tr>
    <tr>
      <td>Nino</td>
      <td>6</td>
      <td> </td>
      <td>Princess</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Max</td>
      <td>6</td>
      <td> </td>
      <td>Lola</td>
      <td>6</td>
    </tr>
    <tr>
      <td>Zeus</td>
      <td>6</td>
      <td> </td>
      <td>Sasha</td>
      <td>6</td>
    </tr>
  </tbody>
</table>

<p>Rambo, my favorite name on this list, is shared by four lucky dogs. Those who enjoy trademark infringement may consider “Oreo,” which is shared by five males and one female.</p>

<p>So what else do we know about these dogs?</p>

<p>For starters, they are slightly more likely to be male, and both males and females (that are registered with DC) are very likely to be fixed.</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/dogs_registered_mf.png" alt="" /></p>

<p>They also have a great chance of being some breed of terrier. More than a third of registered dogs is some kind of terrier or mix. The most common breed variety in DC is the Terrier / Pit Bull mix, with 233 registrants.</p>

<p>Dogs in DC are also on the younger side, as on this chart (measured in people years).</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/dog_years_dc.jpg" alt="" /></p>

<p>The data on who the dogs are in DC comes from a <a href="https://github.com/katerabinowitz/FOIA-Requests/tree/master/Registered%20Dogs">dataset</a> FOIA’d and compiled by the great <a href="http://www.datalensdc.com/">Kate Rabinowitz</a>.</p>

<p>Hopefully, this has been a good excuse for you to go out and pet a pup!</p>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/ncaa-rd1/">The wild predictive ride of the NCAA first round</a>
          </h1>

          <p class="post-meta">Mar 20, 2017</p>
        </header>

        <div class="post-content">
          <p>Ah, March.</p>

<p>It’s a time of bright <a href="http://www.accuweather.com/en/weather-news/noreaster-shuts-down-travel-threatens-to-unleash-blizzard-conditions-in-at-least-8-states/70001093">spring sunshine</a>, cherry blossoms, and people everywhere searching their memories for where exactly <a href="https://en.wikipedia.org/wiki/Xavier_University" title="Cincinnati">Xavier University</a> is.</p>

<p>It’s tournament season! Or at least it is every year in my household until UNC loses. But beyond my general interest in watching a round ball go through a hoop, it’s also a great time to test out predictions. Since everyone and their brother makes predictions about who is likely to win the games each round, I thought I’d check and see how they are doing.</p>

<p>To do that, I simulated the projections from data scientists at <a href="http://www.cbssports.com/college-basketball/bracketology/">CBS Sports</a>, <a href="https://sports.yahoo.com/m/66607537-5012-36a0-8694-65a0522cf6c1/ss_2017-ncaa-tournament-bracket.html">Yahoo!</a>, data <a href="https://projects.fivethirtyeight.com/2017-march-madness-predictions/">blog 538</a>, <a href="http://www.espn.com/mens-college-basketball/bracketology">ESPN</a>, college basketball stat-tracker <a href="http://kenpom.com/">Ken Pomeroy</a>, and sports handicapper <a href="http://sagarin.com/sports/cbsend.htm">Jeff Sagarin</a>.</p>

<p>I collected the percentage likelihoods for each of the rounds from where they were compiled at the <a href="https://www.nytimes.com/interactive/2017/03/13/upshot/ncaa-bracket-super-table.html">NY Times Upshot</a>.
<br /></p>
<details><summary>More methodology</summary>
I then created a simulator that used the projections given, and for each school, simulated 5,000 iterations of each team's probability. I compiled the results of these simulations into an overall projection that differs very slightly (and randomly) from the average of the range of probability projections from a school.</details>
<p><br />
Since the first round of the tournament is 64 hours of basketball shoved into a two-day period, it’s often the most exciting period, and the best for analyzing predictions. So I compiled the results of the first round and compared the scores for each team to the prior prediction.</p>

<p>As the graph below demonstrates, these results split out into four groups:</p>
<ul>
  <li><strong>Favorites</strong>: teams that are performing as advertised; they were supposed to win, and they did win</li>
  <li><strong>Upsets</strong>: this is the reason we watch March Madness; these are the teams that weren’t supposed to win, and then they do. The farther to the left they appear, the more surprising it was.</li>
  <li><strong>Shockers</strong>: this is where the <a href="http://screengrabber.deadspin.com/piccolo-tears-are-the-saddest-tears-1692898303">crying</a> happens; these teams were supposed to win, but lost. Brutal.</li>
  <li><strong>Made-the-Dancers</strong>: where it was an honor just to be nominated, but nobody expected them to win, and they didn’t.</li>
</ul>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/NCAA_rd1_labeled.png" alt="" /></p>

<p>As we move into the second weekend of the tournament, I’ll keep taking a look at how these predictions held up. There are some early upsets. Last year’s champ Villanova have already lost this year. I felt bad for them for exactly <a href="https://www.youtube.com/watch?v=EMHoGRp1QrE">4.7 seconds</a>.</p>

        </div>
        
      </li>
    
  </ul>

  
  <div class="pagination">
    

    
      <a class="next" href="/">Newer &raquo;</a>
    
  </div>



</div>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
    <p>
      


&copy; Brian Austin - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>


</footer>


  </body>

</html>
