<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Overfitting the Sweet 16</title>
  <meta name="description" content="I have a very simple model for about a quarter of my NCAA bracket predictions: if UNC, move to next round. This year, that model has worked pretty well. But unfortunately, it only helps you out in a few circumstances. Every pick is UNC, which is useful when UNC plays Butler, but not so much when Michigan plays Oregon. So since it gives you the same output every time, no matter who is playing, statisticians and data scientists would say this model has low variance. This is isn’t very useful for the broader world of NCAA tournament basketball, so we need a more complex model, one that describes more teams. But here’s the problem a lot of people have in building models: they use everything they know. Let me explain. Here is the bracket of the Sweet 16 games: Since we currently live in a world where we know who won the Sweet 16 games, we can make a super accurate model to “predict” the winners of those games. It might look like this: If UNC is playing, pick UNC (this is my model, after all) If a school has multiple NCAA championships over the last 20 years, pick it if it has more than the other school In a contest between schools in states near large bodies of saltwater and freshwater, pick saltwater If teams are both from the Confederacy, pick the eastern-most team If a school has an X in its name, pick it (Xs are cool) If all else fails, chalk (i.e., pick the team with the lower seed) And we’ll say that these are applied in this order, so that the 1st element of the model trumps the 2nd and so on. Here is how our picks look for the Sweet 16. It’s a weird model, but I’ve got a good feeling about it. Factor Factor Description Team Picked 1. Pick UNC UNC 2. NCAA champions Florida, Kentucky 3. Saltwater over freshwater Florida, Oregon 4. Easternmost Confederate S. Carolina 5. Has an X Xavier 6. Chalk Kansas, Gonzaga How’d we do? Crushed it. We got them all right! We are 100% accurate. This is the best model of all time. So we confidently deploy it to predict the winners of the Elite Eight who will move on to the Final Four. Factor Factor Description Team Picked 1. Pick UNC UNC 2. NCAA champions Florida 3. Saltwater over freshwater N/A, freshwaters already eliminated 4. Easternmost Confederate N/A, Florida already picked 5. Has an X Xavier 6. Chalk Kansas And just as we suspected… Wait what? The only school we got right here was UNC, giving our model an accuracy rate of 25%. Dang. What went wrong? When we went to create a model, we focused on hitting all the points we knew we needed to hit, which statisticians and data scientists refer to as having high bias. This is a big problem in data science, known as “overfitting.” All that means is that predictions overly closely predict the original dataset, and aren’t flexible enough to be applied in the world. So in summary: predictive accuracy comes at a tradeoff, called bias. Simpler models minimize this bias, but run into limitations to accuracy due to low variance. Good models are those that minimize both aspects of this. And Go Heels.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2017/04/03/ncaa_overfitting/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Data futures" href="http://localhost:4000/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="bda">
  <meta name="twitter:title" content="Overfitting the Sweet 16">
  <meta name="twitter:description" content="I have a very simple model for about a quarter of my NCAA bracket predictions: if UNC, move to next round. This year, that model has worked pretty well. But unfortunately, it only helps you out in ...">
  
    <meta name="twitter:creator" content="bda">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-NNNNNNNN-N', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Data futures</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/portfolio/">Portfolio</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/austinbrian">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Overfitting the Sweet 16</h1>
    
    <p class="post-meta"><time datetime="2017-04-03T00:00:00-05:00" itemprop="datePublished">Apr 3, 2017</time> • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Brian Austin</span></span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>I have a very simple model for about a quarter of my NCAA bracket predictions: if UNC, move to next round.</p>

<p>This year, that model has worked pretty well.</p>

<p>But unfortunately, it only helps you out in a few circumstances. Every pick is UNC, which is useful when UNC plays Butler, but not so much when Michigan plays Oregon. So since it gives you the same output every time, no matter who is playing, statisticians and data scientists would say this model has <strong>low variance</strong>.</p>

<p>This is isn’t very useful for the broader world of NCAA tournament basketball, so we need a more complex model, one that describes more teams. But here’s the problem a lot of people have in building models: <em>they use everything they know</em>. Let me explain.</p>

<p>Here is the bracket of the Sweet 16 games:</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/ncaa_ss_bracket_8blank.png" alt="where's Dook?" /></p>

<p>Since we currently live in a world where we know who won the Sweet 16 games, we can make a <em>super</em> accurate model to “predict” the winners of those games. It might look like this:</p>
<ol>
  <li>If UNC is playing, pick UNC (this is my model, after all)</li>
  <li>If a school has multiple NCAA championships over the last 20 years, pick it if it has more than the other school</li>
  <li>In a contest between schools in states near large bodies of saltwater and freshwater, pick saltwater</li>
  <li>If teams are both from the Confederacy, pick the eastern-most team</li>
  <li>If a school has an X in its name, pick it (<em>Xs are cool</em>)</li>
  <li>If all else fails, chalk (i.e., pick the team with the lower seed)</li>
</ol>

<p>And we’ll say that these are applied in this order, so that the 1st element of the model trumps the 2nd and so on.</p>

<p>Here is how our picks look for the Sweet 16. It’s a weird model, but I’ve got a good feeling about it.</p>

<table>
  <thead>
    <tr>
      <th>Factor</th>
      <th>Factor Description</th>
      <th>Team Picked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.</td>
      <td>Pick UNC</td>
      <td>UNC</td>
    </tr>
    <tr>
      <td>2.</td>
      <td>NCAA champions</td>
      <td>Florida, Kentucky</td>
    </tr>
    <tr>
      <td>3.</td>
      <td>Saltwater over freshwater</td>
      <td>Florida, Oregon</td>
    </tr>
    <tr>
      <td>4.</td>
      <td>Easternmost Confederate</td>
      <td>S. Carolina</td>
    </tr>
    <tr>
      <td>5.</td>
      <td>Has an X</td>
      <td>Xavier</td>
    </tr>
    <tr>
      <td>6.</td>
      <td>Chalk</td>
      <td>Kansas, Gonzaga</td>
    </tr>
  </tbody>
</table>

<p>How’d we do?</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/NCAA_ss_circles.png" alt="wow such prediction" /></p>

<p>Crushed it.</p>

<p>We got them all right! We are 100% accurate. This is the best model of all time. So we confidently deploy it to predict the winners of the Elite Eight who will move on to the Final Four.</p>

<table>
  <thead>
    <tr>
      <th>Factor</th>
      <th>Factor Description</th>
      <th>Team Picked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.</td>
      <td>Pick UNC</td>
      <td>UNC</td>
    </tr>
    <tr>
      <td>2.</td>
      <td>NCAA champions</td>
      <td>Florida</td>
    </tr>
    <tr>
      <td>3.</td>
      <td>Saltwater over freshwater</td>
      <td><em>N/A, freshwaters already eliminated</em></td>
    </tr>
    <tr>
      <td>4.</td>
      <td>Easternmost Confederate</td>
      <td><em>N/A, Florida already picked</em></td>
    </tr>
    <tr>
      <td>5.</td>
      <td>Has an X</td>
      <td>Xavier</td>
    </tr>
    <tr>
      <td>6.</td>
      <td>Chalk</td>
      <td>Kansas</td>
    </tr>
  </tbody>
</table>

<p>And just as we suspected…</p>

<p><img src="https://raw.githubusercontent.com/austinbrian/blog/master/images/NCAA_ss_xxxs.png" alt="not as good" /></p>

<p>Wait what?</p>

<p>The only school we got right here was UNC, giving our model an accuracy rate of 25%. Dang.</p>

<p>What went wrong?</p>

<p>When we went to create a model, we focused on hitting all the points we knew we needed to hit, which statisticians and data scientists refer to as having <strong>high bias</strong>.</p>

<p>This is a big problem in data science, known as “overfitting.” All that means is that predictions overly closely predict the original dataset, and aren’t flexible enough to be applied in the world.</p>

<p>So in summary: predictive accuracy comes at a tradeoff, called <strong>bias</strong>. Simpler models minimize this bias, but run into limitations to accuracy due to low <strong>variance</strong>. Good models are those that minimize both aspects of this.</p>

<p>And Go Heels.</p>

  </div>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Brian Austin - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
